{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.251414204902577,
  "eval_steps": 500,
  "global_step": 1400,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0017958157493041214,
      "grad_norm": 0.5802289247512817,
      "learning_rate": 0.00019910000000000001,
      "loss": 1.8185,
      "step": 10
    },
    {
      "epoch": 0.003591631498608243,
      "grad_norm": 0.44251078367233276,
      "learning_rate": 0.00019810000000000002,
      "loss": 0.9879,
      "step": 20
    },
    {
      "epoch": 0.005387447247912364,
      "grad_norm": 0.3062489330768585,
      "learning_rate": 0.00019710000000000002,
      "loss": 0.8621,
      "step": 30
    },
    {
      "epoch": 0.007183262997216486,
      "grad_norm": 0.30070674419403076,
      "learning_rate": 0.00019610000000000002,
      "loss": 0.8089,
      "step": 40
    },
    {
      "epoch": 0.008979078746520607,
      "grad_norm": 0.33556970953941345,
      "learning_rate": 0.00019510000000000003,
      "loss": 0.8264,
      "step": 50
    },
    {
      "epoch": 0.010774894495824729,
      "grad_norm": 0.24950695037841797,
      "learning_rate": 0.0001941,
      "loss": 0.7592,
      "step": 60
    },
    {
      "epoch": 0.01257071024512885,
      "grad_norm": 0.242641419172287,
      "learning_rate": 0.0001931,
      "loss": 0.7534,
      "step": 70
    },
    {
      "epoch": 0.014366525994432972,
      "grad_norm": 0.2349753975868225,
      "learning_rate": 0.0001921,
      "loss": 0.7558,
      "step": 80
    },
    {
      "epoch": 0.016162341743737093,
      "grad_norm": 0.2486787736415863,
      "learning_rate": 0.0001911,
      "loss": 0.7357,
      "step": 90
    },
    {
      "epoch": 0.017958157493041214,
      "grad_norm": 0.24199913442134857,
      "learning_rate": 0.0001901,
      "loss": 0.7197,
      "step": 100
    },
    {
      "epoch": 0.019753973242345336,
      "grad_norm": 0.2536809742450714,
      "learning_rate": 0.00018910000000000002,
      "loss": 0.75,
      "step": 110
    },
    {
      "epoch": 0.021549788991649457,
      "grad_norm": 0.24645157158374786,
      "learning_rate": 0.00018810000000000002,
      "loss": 0.7136,
      "step": 120
    },
    {
      "epoch": 0.02334560474095358,
      "grad_norm": 0.26772263646125793,
      "learning_rate": 0.00018710000000000002,
      "loss": 0.6778,
      "step": 130
    },
    {
      "epoch": 0.0251414204902577,
      "grad_norm": 0.2775100767612457,
      "learning_rate": 0.0001861,
      "loss": 0.6685,
      "step": 140
    },
    {
      "epoch": 0.02693723623956182,
      "grad_norm": 0.28745177388191223,
      "learning_rate": 0.0001851,
      "loss": 0.676,
      "step": 150
    },
    {
      "epoch": 0.028733051988865943,
      "grad_norm": 0.3191547095775604,
      "learning_rate": 0.0001841,
      "loss": 0.6853,
      "step": 160
    },
    {
      "epoch": 0.030528867738170064,
      "grad_norm": 0.28210368752479553,
      "learning_rate": 0.0001831,
      "loss": 0.6437,
      "step": 170
    },
    {
      "epoch": 0.032324683487474186,
      "grad_norm": 0.2665787637233734,
      "learning_rate": 0.0001821,
      "loss": 0.6375,
      "step": 180
    },
    {
      "epoch": 0.034120499236778304,
      "grad_norm": 0.2871152460575104,
      "learning_rate": 0.0001811,
      "loss": 0.6454,
      "step": 190
    },
    {
      "epoch": 0.03591631498608243,
      "grad_norm": 0.2725357115268707,
      "learning_rate": 0.00018010000000000001,
      "loss": 0.6651,
      "step": 200
    },
    {
      "epoch": 0.03771213073538655,
      "grad_norm": 0.29926303029060364,
      "learning_rate": 0.0001791,
      "loss": 0.6569,
      "step": 210
    },
    {
      "epoch": 0.03950794648469067,
      "grad_norm": 0.31479382514953613,
      "learning_rate": 0.0001781,
      "loss": 0.6528,
      "step": 220
    },
    {
      "epoch": 0.04130376223399479,
      "grad_norm": 0.3129783570766449,
      "learning_rate": 0.0001771,
      "loss": 0.6164,
      "step": 230
    },
    {
      "epoch": 0.043099577983298915,
      "grad_norm": 0.25674495100975037,
      "learning_rate": 0.0001761,
      "loss": 0.6423,
      "step": 240
    },
    {
      "epoch": 0.04489539373260303,
      "grad_norm": 0.2818242311477661,
      "learning_rate": 0.0001751,
      "loss": 0.6169,
      "step": 250
    },
    {
      "epoch": 0.04669120948190716,
      "grad_norm": 0.2761869728565216,
      "learning_rate": 0.00017410000000000003,
      "loss": 0.6063,
      "step": 260
    },
    {
      "epoch": 0.048487025231211275,
      "grad_norm": 0.41302919387817383,
      "learning_rate": 0.0001731,
      "loss": 0.6201,
      "step": 270
    },
    {
      "epoch": 0.0502828409805154,
      "grad_norm": 0.29827016592025757,
      "learning_rate": 0.0001721,
      "loss": 0.5769,
      "step": 280
    },
    {
      "epoch": 0.05207865672981952,
      "grad_norm": 0.28158414363861084,
      "learning_rate": 0.0001711,
      "loss": 0.5851,
      "step": 290
    },
    {
      "epoch": 0.05387447247912364,
      "grad_norm": 0.24212223291397095,
      "learning_rate": 0.00017010000000000001,
      "loss": 0.5577,
      "step": 300
    },
    {
      "epoch": 0.05567028822842776,
      "grad_norm": 0.2846154272556305,
      "learning_rate": 0.00016910000000000002,
      "loss": 0.5903,
      "step": 310
    },
    {
      "epoch": 0.057466103977731886,
      "grad_norm": 0.30123233795166016,
      "learning_rate": 0.00016810000000000002,
      "loss": 0.6058,
      "step": 320
    },
    {
      "epoch": 0.059261919727036004,
      "grad_norm": 0.30532532930374146,
      "learning_rate": 0.00016710000000000002,
      "loss": 0.5844,
      "step": 330
    },
    {
      "epoch": 0.06105773547634013,
      "grad_norm": 0.30380725860595703,
      "learning_rate": 0.0001661,
      "loss": 0.5774,
      "step": 340
    },
    {
      "epoch": 0.06285355122564425,
      "grad_norm": 0.2719590365886688,
      "learning_rate": 0.0001651,
      "loss": 0.5625,
      "step": 350
    },
    {
      "epoch": 0.06464936697494837,
      "grad_norm": 0.2885153889656067,
      "learning_rate": 0.0001641,
      "loss": 0.5681,
      "step": 360
    },
    {
      "epoch": 0.0664451827242525,
      "grad_norm": 0.32683882117271423,
      "learning_rate": 0.0001631,
      "loss": 0.5659,
      "step": 370
    },
    {
      "epoch": 0.06824099847355661,
      "grad_norm": 0.27465179562568665,
      "learning_rate": 0.0001621,
      "loss": 0.5712,
      "step": 380
    },
    {
      "epoch": 0.07003681422286073,
      "grad_norm": 0.2850980758666992,
      "learning_rate": 0.0001611,
      "loss": 0.56,
      "step": 390
    },
    {
      "epoch": 0.07183262997216486,
      "grad_norm": 0.48596397042274475,
      "learning_rate": 0.00016010000000000002,
      "loss": 0.5525,
      "step": 400
    },
    {
      "epoch": 0.07362844572146898,
      "grad_norm": 0.2709933817386627,
      "learning_rate": 0.0001591,
      "loss": 0.5338,
      "step": 410
    },
    {
      "epoch": 0.0754242614707731,
      "grad_norm": 0.28693342208862305,
      "learning_rate": 0.0001581,
      "loss": 0.5232,
      "step": 420
    },
    {
      "epoch": 0.07722007722007722,
      "grad_norm": 0.28502434492111206,
      "learning_rate": 0.0001571,
      "loss": 0.5185,
      "step": 430
    },
    {
      "epoch": 0.07901589296938134,
      "grad_norm": 0.36458414793014526,
      "learning_rate": 0.0001561,
      "loss": 0.5316,
      "step": 440
    },
    {
      "epoch": 0.08081170871868547,
      "grad_norm": 0.3723112642765045,
      "learning_rate": 0.0001551,
      "loss": 0.5495,
      "step": 450
    },
    {
      "epoch": 0.08260752446798958,
      "grad_norm": 0.344698041677475,
      "learning_rate": 0.0001541,
      "loss": 0.5352,
      "step": 460
    },
    {
      "epoch": 0.0844033402172937,
      "grad_norm": 0.3733258545398712,
      "learning_rate": 0.0001531,
      "loss": 0.5067,
      "step": 470
    },
    {
      "epoch": 0.08619915596659783,
      "grad_norm": 0.27006036043167114,
      "learning_rate": 0.0001521,
      "loss": 0.5343,
      "step": 480
    },
    {
      "epoch": 0.08799497171590195,
      "grad_norm": 0.2627566158771515,
      "learning_rate": 0.0001511,
      "loss": 0.5198,
      "step": 490
    },
    {
      "epoch": 0.08979078746520607,
      "grad_norm": 0.312785267829895,
      "learning_rate": 0.0001501,
      "loss": 0.5082,
      "step": 500
    },
    {
      "epoch": 0.09158660321451019,
      "grad_norm": 0.30332517623901367,
      "learning_rate": 0.00014910000000000002,
      "loss": 0.5406,
      "step": 510
    },
    {
      "epoch": 0.09338241896381431,
      "grad_norm": 0.2888644337654114,
      "learning_rate": 0.00014810000000000002,
      "loss": 0.5012,
      "step": 520
    },
    {
      "epoch": 0.09517823471311844,
      "grad_norm": 0.30949291586875916,
      "learning_rate": 0.00014710000000000002,
      "loss": 0.5274,
      "step": 530
    },
    {
      "epoch": 0.09697405046242255,
      "grad_norm": 0.2764213979244232,
      "learning_rate": 0.00014610000000000003,
      "loss": 0.5158,
      "step": 540
    },
    {
      "epoch": 0.09876986621172668,
      "grad_norm": 0.28510186076164246,
      "learning_rate": 0.0001451,
      "loss": 0.5146,
      "step": 550
    },
    {
      "epoch": 0.1005656819610308,
      "grad_norm": 0.408192902803421,
      "learning_rate": 0.0001441,
      "loss": 0.5125,
      "step": 560
    },
    {
      "epoch": 0.10236149771033493,
      "grad_norm": 0.29953935742378235,
      "learning_rate": 0.0001431,
      "loss": 0.502,
      "step": 570
    },
    {
      "epoch": 0.10415731345963904,
      "grad_norm": 0.30940166115760803,
      "learning_rate": 0.0001421,
      "loss": 0.5075,
      "step": 580
    },
    {
      "epoch": 0.10595312920894316,
      "grad_norm": 0.31743934750556946,
      "learning_rate": 0.00014110000000000001,
      "loss": 0.4859,
      "step": 590
    },
    {
      "epoch": 0.10774894495824729,
      "grad_norm": 0.28316888213157654,
      "learning_rate": 0.00014010000000000002,
      "loss": 0.4979,
      "step": 600
    },
    {
      "epoch": 0.10954476070755141,
      "grad_norm": 0.30762454867362976,
      "learning_rate": 0.00013910000000000002,
      "loss": 0.4895,
      "step": 610
    },
    {
      "epoch": 0.11134057645685552,
      "grad_norm": 0.3061901032924652,
      "learning_rate": 0.0001381,
      "loss": 0.5029,
      "step": 620
    },
    {
      "epoch": 0.11313639220615965,
      "grad_norm": 1.5389277935028076,
      "learning_rate": 0.0001371,
      "loss": 0.5146,
      "step": 630
    },
    {
      "epoch": 0.11493220795546377,
      "grad_norm": 0.3060114085674286,
      "learning_rate": 0.0001361,
      "loss": 0.4919,
      "step": 640
    },
    {
      "epoch": 0.1167280237047679,
      "grad_norm": 0.28834134340286255,
      "learning_rate": 0.0001351,
      "loss": 0.4773,
      "step": 650
    },
    {
      "epoch": 0.11852383945407201,
      "grad_norm": 0.2722896337509155,
      "learning_rate": 0.0001341,
      "loss": 0.5039,
      "step": 660
    },
    {
      "epoch": 0.12031965520337613,
      "grad_norm": 0.3282575309276581,
      "learning_rate": 0.0001331,
      "loss": 0.4767,
      "step": 670
    },
    {
      "epoch": 0.12211547095268026,
      "grad_norm": 0.29462888836860657,
      "learning_rate": 0.0001321,
      "loss": 0.4939,
      "step": 680
    },
    {
      "epoch": 0.12391128670198438,
      "grad_norm": 0.3104342520236969,
      "learning_rate": 0.0001311,
      "loss": 0.4844,
      "step": 690
    },
    {
      "epoch": 0.1257071024512885,
      "grad_norm": 0.31587058305740356,
      "learning_rate": 0.0001301,
      "loss": 0.4602,
      "step": 700
    },
    {
      "epoch": 0.12750291820059262,
      "grad_norm": 0.28590258955955505,
      "learning_rate": 0.0001291,
      "loss": 0.4717,
      "step": 710
    },
    {
      "epoch": 0.12929873394989674,
      "grad_norm": 0.29165181517601013,
      "learning_rate": 0.0001281,
      "loss": 0.477,
      "step": 720
    },
    {
      "epoch": 0.13109454969920087,
      "grad_norm": 0.288998007774353,
      "learning_rate": 0.0001271,
      "loss": 0.4664,
      "step": 730
    },
    {
      "epoch": 0.132890365448505,
      "grad_norm": 0.30232125520706177,
      "learning_rate": 0.0001261,
      "loss": 0.4774,
      "step": 740
    },
    {
      "epoch": 0.13468618119780912,
      "grad_norm": 0.33501094579696655,
      "learning_rate": 0.0001251,
      "loss": 0.4879,
      "step": 750
    },
    {
      "epoch": 0.13648199694711322,
      "grad_norm": 0.3068402111530304,
      "learning_rate": 0.0001241,
      "loss": 0.4544,
      "step": 760
    },
    {
      "epoch": 0.13827781269641734,
      "grad_norm": 0.27859628200531006,
      "learning_rate": 0.0001231,
      "loss": 0.4629,
      "step": 770
    },
    {
      "epoch": 0.14007362844572147,
      "grad_norm": 0.31963691115379333,
      "learning_rate": 0.0001221,
      "loss": 0.449,
      "step": 780
    },
    {
      "epoch": 0.1418694441950256,
      "grad_norm": 0.29499077796936035,
      "learning_rate": 0.00012110000000000002,
      "loss": 0.4561,
      "step": 790
    },
    {
      "epoch": 0.14366525994432972,
      "grad_norm": 0.2822263836860657,
      "learning_rate": 0.00012010000000000002,
      "loss": 0.4715,
      "step": 800
    },
    {
      "epoch": 0.14546107569363384,
      "grad_norm": 0.2933315336704254,
      "learning_rate": 0.00011910000000000001,
      "loss": 0.4424,
      "step": 810
    },
    {
      "epoch": 0.14725689144293796,
      "grad_norm": 0.3061903417110443,
      "learning_rate": 0.00011810000000000001,
      "loss": 0.4423,
      "step": 820
    },
    {
      "epoch": 0.14905270719224206,
      "grad_norm": 0.3395719528198242,
      "learning_rate": 0.00011710000000000001,
      "loss": 0.451,
      "step": 830
    },
    {
      "epoch": 0.1508485229415462,
      "grad_norm": 0.2728956639766693,
      "learning_rate": 0.0001161,
      "loss": 0.4608,
      "step": 840
    },
    {
      "epoch": 0.1526443386908503,
      "grad_norm": 0.29476791620254517,
      "learning_rate": 0.0001151,
      "loss": 0.4655,
      "step": 850
    },
    {
      "epoch": 0.15444015444015444,
      "grad_norm": 0.3198665976524353,
      "learning_rate": 0.00011410000000000001,
      "loss": 0.4518,
      "step": 860
    },
    {
      "epoch": 0.15623597018945856,
      "grad_norm": 0.29230648279190063,
      "learning_rate": 0.00011310000000000001,
      "loss": 0.4541,
      "step": 870
    },
    {
      "epoch": 0.1580317859387627,
      "grad_norm": 0.2879215180873871,
      "learning_rate": 0.0001121,
      "loss": 0.45,
      "step": 880
    },
    {
      "epoch": 0.1598276016880668,
      "grad_norm": 0.2767261862754822,
      "learning_rate": 0.0001111,
      "loss": 0.474,
      "step": 890
    },
    {
      "epoch": 0.16162341743737094,
      "grad_norm": 0.28345224261283875,
      "learning_rate": 0.0001101,
      "loss": 0.4366,
      "step": 900
    },
    {
      "epoch": 0.16341923318667503,
      "grad_norm": 0.2809453010559082,
      "learning_rate": 0.0001091,
      "loss": 0.4397,
      "step": 910
    },
    {
      "epoch": 0.16521504893597916,
      "grad_norm": 0.33756327629089355,
      "learning_rate": 0.0001081,
      "loss": 0.4551,
      "step": 920
    },
    {
      "epoch": 0.16701086468528328,
      "grad_norm": 0.2605198621749878,
      "learning_rate": 0.0001071,
      "loss": 0.4173,
      "step": 930
    },
    {
      "epoch": 0.1688066804345874,
      "grad_norm": 0.29075297713279724,
      "learning_rate": 0.0001061,
      "loss": 0.4645,
      "step": 940
    },
    {
      "epoch": 0.17060249618389153,
      "grad_norm": 0.35599198937416077,
      "learning_rate": 0.0001051,
      "loss": 0.4517,
      "step": 950
    },
    {
      "epoch": 0.17239831193319566,
      "grad_norm": 0.31170350313186646,
      "learning_rate": 0.0001041,
      "loss": 0.4528,
      "step": 960
    },
    {
      "epoch": 0.17419412768249978,
      "grad_norm": 0.3301677107810974,
      "learning_rate": 0.0001031,
      "loss": 0.436,
      "step": 970
    },
    {
      "epoch": 0.1759899434318039,
      "grad_norm": 0.3177535831928253,
      "learning_rate": 0.0001021,
      "loss": 0.4284,
      "step": 980
    },
    {
      "epoch": 0.177785759181108,
      "grad_norm": 0.23674073815345764,
      "learning_rate": 0.00010109999999999999,
      "loss": 0.4505,
      "step": 990
    },
    {
      "epoch": 0.17958157493041213,
      "grad_norm": 0.25052738189697266,
      "learning_rate": 0.0001001,
      "loss": 0.445,
      "step": 1000
    },
    {
      "epoch": 0.18137739067971625,
      "grad_norm": 0.3328878879547119,
      "learning_rate": 9.910000000000001e-05,
      "loss": 0.4355,
      "step": 1010
    },
    {
      "epoch": 0.18317320642902038,
      "grad_norm": 0.2959368824958801,
      "learning_rate": 9.81e-05,
      "loss": 0.4314,
      "step": 1020
    },
    {
      "epoch": 0.1849690221783245,
      "grad_norm": 0.2848851978778839,
      "learning_rate": 9.71e-05,
      "loss": 0.4336,
      "step": 1030
    },
    {
      "epoch": 0.18676483792762863,
      "grad_norm": 0.3262745141983032,
      "learning_rate": 9.61e-05,
      "loss": 0.4409,
      "step": 1040
    },
    {
      "epoch": 0.18856065367693275,
      "grad_norm": 0.30469417572021484,
      "learning_rate": 9.51e-05,
      "loss": 0.4321,
      "step": 1050
    },
    {
      "epoch": 0.19035646942623688,
      "grad_norm": 0.34865957498550415,
      "learning_rate": 9.41e-05,
      "loss": 0.4298,
      "step": 1060
    },
    {
      "epoch": 0.19215228517554098,
      "grad_norm": 0.3144562542438507,
      "learning_rate": 9.310000000000001e-05,
      "loss": 0.4393,
      "step": 1070
    },
    {
      "epoch": 0.1939481009248451,
      "grad_norm": 0.2743666172027588,
      "learning_rate": 9.21e-05,
      "loss": 0.4171,
      "step": 1080
    },
    {
      "epoch": 0.19574391667414923,
      "grad_norm": 0.3148343563079834,
      "learning_rate": 9.11e-05,
      "loss": 0.4358,
      "step": 1090
    },
    {
      "epoch": 0.19753973242345335,
      "grad_norm": 0.27570775151252747,
      "learning_rate": 9.010000000000001e-05,
      "loss": 0.4262,
      "step": 1100
    },
    {
      "epoch": 0.19933554817275748,
      "grad_norm": 0.2574164867401123,
      "learning_rate": 8.910000000000001e-05,
      "loss": 0.4071,
      "step": 1110
    },
    {
      "epoch": 0.2011313639220616,
      "grad_norm": 0.31740933656692505,
      "learning_rate": 8.81e-05,
      "loss": 0.4431,
      "step": 1120
    },
    {
      "epoch": 0.20292717967136573,
      "grad_norm": 0.3028809726238251,
      "learning_rate": 8.71e-05,
      "loss": 0.4157,
      "step": 1130
    },
    {
      "epoch": 0.20472299542066985,
      "grad_norm": 0.3274935781955719,
      "learning_rate": 8.61e-05,
      "loss": 0.4303,
      "step": 1140
    },
    {
      "epoch": 0.20651881116997395,
      "grad_norm": 0.30387914180755615,
      "learning_rate": 8.510000000000001e-05,
      "loss": 0.4355,
      "step": 1150
    },
    {
      "epoch": 0.20831462691927807,
      "grad_norm": 0.29430723190307617,
      "learning_rate": 8.41e-05,
      "loss": 0.4333,
      "step": 1160
    },
    {
      "epoch": 0.2101104426685822,
      "grad_norm": 0.23882484436035156,
      "learning_rate": 8.31e-05,
      "loss": 0.4419,
      "step": 1170
    },
    {
      "epoch": 0.21190625841788632,
      "grad_norm": 0.287189781665802,
      "learning_rate": 8.21e-05,
      "loss": 0.4312,
      "step": 1180
    },
    {
      "epoch": 0.21370207416719045,
      "grad_norm": 0.307774156332016,
      "learning_rate": 8.11e-05,
      "loss": 0.4412,
      "step": 1190
    },
    {
      "epoch": 0.21549788991649457,
      "grad_norm": 0.32668745517730713,
      "learning_rate": 8.010000000000001e-05,
      "loss": 0.425,
      "step": 1200
    },
    {
      "epoch": 0.2172937056657987,
      "grad_norm": 0.263275146484375,
      "learning_rate": 7.910000000000001e-05,
      "loss": 0.4145,
      "step": 1210
    },
    {
      "epoch": 0.21908952141510282,
      "grad_norm": 0.43201225996017456,
      "learning_rate": 7.81e-05,
      "loss": 0.4287,
      "step": 1220
    },
    {
      "epoch": 0.22088533716440692,
      "grad_norm": 0.2996560335159302,
      "learning_rate": 7.71e-05,
      "loss": 0.4346,
      "step": 1230
    },
    {
      "epoch": 0.22268115291371104,
      "grad_norm": 0.26242026686668396,
      "learning_rate": 7.61e-05,
      "loss": 0.4005,
      "step": 1240
    },
    {
      "epoch": 0.22447696866301517,
      "grad_norm": 0.28518274426460266,
      "learning_rate": 7.510000000000001e-05,
      "loss": 0.4236,
      "step": 1250
    },
    {
      "epoch": 0.2262727844123193,
      "grad_norm": 0.27448970079421997,
      "learning_rate": 7.41e-05,
      "loss": 0.4035,
      "step": 1260
    },
    {
      "epoch": 0.22806860016162342,
      "grad_norm": 0.26342856884002686,
      "learning_rate": 7.31e-05,
      "loss": 0.4027,
      "step": 1270
    },
    {
      "epoch": 0.22986441591092754,
      "grad_norm": 0.33526644110679626,
      "learning_rate": 7.21e-05,
      "loss": 0.4158,
      "step": 1280
    },
    {
      "epoch": 0.23166023166023167,
      "grad_norm": 0.28030043840408325,
      "learning_rate": 7.11e-05,
      "loss": 0.4228,
      "step": 1290
    },
    {
      "epoch": 0.2334560474095358,
      "grad_norm": 0.3381297290325165,
      "learning_rate": 7.01e-05,
      "loss": 0.4171,
      "step": 1300
    },
    {
      "epoch": 0.2352518631588399,
      "grad_norm": 0.40914836525917053,
      "learning_rate": 6.91e-05,
      "loss": 0.4352,
      "step": 1310
    },
    {
      "epoch": 0.23704767890814402,
      "grad_norm": 0.2687309682369232,
      "learning_rate": 6.81e-05,
      "loss": 0.3951,
      "step": 1320
    },
    {
      "epoch": 0.23884349465744814,
      "grad_norm": 0.3354613780975342,
      "learning_rate": 6.71e-05,
      "loss": 0.4282,
      "step": 1330
    },
    {
      "epoch": 0.24063931040675227,
      "grad_norm": 0.27472421526908875,
      "learning_rate": 6.610000000000001e-05,
      "loss": 0.3963,
      "step": 1340
    },
    {
      "epoch": 0.2424351261560564,
      "grad_norm": 0.27455902099609375,
      "learning_rate": 6.510000000000001e-05,
      "loss": 0.4077,
      "step": 1350
    },
    {
      "epoch": 0.24423094190536052,
      "grad_norm": 0.29004594683647156,
      "learning_rate": 6.41e-05,
      "loss": 0.401,
      "step": 1360
    },
    {
      "epoch": 0.24602675765466464,
      "grad_norm": 0.3341323137283325,
      "learning_rate": 6.31e-05,
      "loss": 0.4154,
      "step": 1370
    },
    {
      "epoch": 0.24782257340396877,
      "grad_norm": 0.3157334327697754,
      "learning_rate": 6.21e-05,
      "loss": 0.4161,
      "step": 1380
    },
    {
      "epoch": 0.24961838915327286,
      "grad_norm": 0.3202669620513916,
      "learning_rate": 6.110000000000001e-05,
      "loss": 0.3939,
      "step": 1390
    },
    {
      "epoch": 0.251414204902577,
      "grad_norm": 0.48285478353500366,
      "learning_rate": 6.0100000000000004e-05,
      "loss": 0.3899,
      "step": 1400
    }
  ],
  "logging_steps": 10,
  "max_steps": 2000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.35072070587777e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
