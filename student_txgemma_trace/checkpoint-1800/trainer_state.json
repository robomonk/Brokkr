{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.36574215178299296,
  "eval_steps": 500,
  "global_step": 1800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00203190084323885,
      "grad_norm": 1.165687084197998,
      "learning_rate": 0.00019910000000000001,
      "loss": 1.9569,
      "step": 10
    },
    {
      "epoch": 0.0040638016864777,
      "grad_norm": 0.2793021500110626,
      "learning_rate": 0.00019810000000000002,
      "loss": 1.0785,
      "step": 20
    },
    {
      "epoch": 0.00609570252971655,
      "grad_norm": 0.2917936444282532,
      "learning_rate": 0.00019710000000000002,
      "loss": 0.9965,
      "step": 30
    },
    {
      "epoch": 0.0081276033729554,
      "grad_norm": 0.3397102355957031,
      "learning_rate": 0.00019610000000000002,
      "loss": 0.9243,
      "step": 40
    },
    {
      "epoch": 0.01015950421619425,
      "grad_norm": 0.2767530083656311,
      "learning_rate": 0.00019510000000000003,
      "loss": 0.8793,
      "step": 50
    },
    {
      "epoch": 0.0121914050594331,
      "grad_norm": 0.26831814646720886,
      "learning_rate": 0.0001941,
      "loss": 0.8664,
      "step": 60
    },
    {
      "epoch": 0.01422330590267195,
      "grad_norm": 0.24407950043678284,
      "learning_rate": 0.0001931,
      "loss": 0.8438,
      "step": 70
    },
    {
      "epoch": 0.0162552067459108,
      "grad_norm": 0.29952120780944824,
      "learning_rate": 0.0001921,
      "loss": 0.7991,
      "step": 80
    },
    {
      "epoch": 0.01828710758914965,
      "grad_norm": 0.25862154364585876,
      "learning_rate": 0.0001911,
      "loss": 0.7997,
      "step": 90
    },
    {
      "epoch": 0.0203190084323885,
      "grad_norm": 0.25606852769851685,
      "learning_rate": 0.0001901,
      "loss": 0.8306,
      "step": 100
    },
    {
      "epoch": 0.02235090927562735,
      "grad_norm": 0.2541874647140503,
      "learning_rate": 0.00018910000000000002,
      "loss": 0.7945,
      "step": 110
    },
    {
      "epoch": 0.0243828101188662,
      "grad_norm": 0.29285746812820435,
      "learning_rate": 0.00018810000000000002,
      "loss": 0.7923,
      "step": 120
    },
    {
      "epoch": 0.02641471096210505,
      "grad_norm": 0.24532675743103027,
      "learning_rate": 0.00018710000000000002,
      "loss": 0.7397,
      "step": 130
    },
    {
      "epoch": 0.0284466118053439,
      "grad_norm": 0.2939009964466095,
      "learning_rate": 0.0001861,
      "loss": 0.7454,
      "step": 140
    },
    {
      "epoch": 0.03047851264858275,
      "grad_norm": 0.28754323720932007,
      "learning_rate": 0.0001851,
      "loss": 0.7084,
      "step": 150
    },
    {
      "epoch": 0.0325104134918216,
      "grad_norm": 0.2901480495929718,
      "learning_rate": 0.0001841,
      "loss": 0.7442,
      "step": 160
    },
    {
      "epoch": 0.03454231433506045,
      "grad_norm": 0.34662312269210815,
      "learning_rate": 0.0001831,
      "loss": 0.7286,
      "step": 170
    },
    {
      "epoch": 0.0365742151782993,
      "grad_norm": 0.306060791015625,
      "learning_rate": 0.0001821,
      "loss": 0.7227,
      "step": 180
    },
    {
      "epoch": 0.03860611602153815,
      "grad_norm": 0.2749321162700653,
      "learning_rate": 0.0001811,
      "loss": 0.7239,
      "step": 190
    },
    {
      "epoch": 0.040638016864777,
      "grad_norm": 0.327809602022171,
      "learning_rate": 0.00018010000000000001,
      "loss": 0.7055,
      "step": 200
    },
    {
      "epoch": 0.04266991770801585,
      "grad_norm": 0.2977617084980011,
      "learning_rate": 0.0001791,
      "loss": 0.6874,
      "step": 210
    },
    {
      "epoch": 0.0447018185512547,
      "grad_norm": 0.29844924807548523,
      "learning_rate": 0.0001781,
      "loss": 0.7184,
      "step": 220
    },
    {
      "epoch": 0.04673371939449355,
      "grad_norm": 0.27778360247612,
      "learning_rate": 0.0001771,
      "loss": 0.6679,
      "step": 230
    },
    {
      "epoch": 0.0487656202377324,
      "grad_norm": 0.284200519323349,
      "learning_rate": 0.0001761,
      "loss": 0.6643,
      "step": 240
    },
    {
      "epoch": 0.05079752108097125,
      "grad_norm": 0.29384177923202515,
      "learning_rate": 0.0001751,
      "loss": 0.6905,
      "step": 250
    },
    {
      "epoch": 0.0528294219242101,
      "grad_norm": 0.29748770594596863,
      "learning_rate": 0.00017410000000000003,
      "loss": 0.6557,
      "step": 260
    },
    {
      "epoch": 0.05486132276744895,
      "grad_norm": 0.2729402482509613,
      "learning_rate": 0.0001731,
      "loss": 0.6382,
      "step": 270
    },
    {
      "epoch": 0.0568932236106878,
      "grad_norm": 0.2993599474430084,
      "learning_rate": 0.0001721,
      "loss": 0.6568,
      "step": 280
    },
    {
      "epoch": 0.05892512445392665,
      "grad_norm": 0.2952386438846588,
      "learning_rate": 0.0001711,
      "loss": 0.6362,
      "step": 290
    },
    {
      "epoch": 0.0609570252971655,
      "grad_norm": 0.26254668831825256,
      "learning_rate": 0.00017010000000000001,
      "loss": 0.6027,
      "step": 300
    },
    {
      "epoch": 0.06298892614040434,
      "grad_norm": 0.35756734013557434,
      "learning_rate": 0.00016910000000000002,
      "loss": 0.6209,
      "step": 310
    },
    {
      "epoch": 0.0650208269836432,
      "grad_norm": 0.331665962934494,
      "learning_rate": 0.00016810000000000002,
      "loss": 0.63,
      "step": 320
    },
    {
      "epoch": 0.06705272782688204,
      "grad_norm": 0.32876911759376526,
      "learning_rate": 0.00016710000000000002,
      "loss": 0.6053,
      "step": 330
    },
    {
      "epoch": 0.0690846286701209,
      "grad_norm": 0.2975154519081116,
      "learning_rate": 0.0001661,
      "loss": 0.6132,
      "step": 340
    },
    {
      "epoch": 0.07111652951335974,
      "grad_norm": 0.32871952652931213,
      "learning_rate": 0.0001651,
      "loss": 0.6056,
      "step": 350
    },
    {
      "epoch": 0.0731484303565986,
      "grad_norm": 0.3313707411289215,
      "learning_rate": 0.0001641,
      "loss": 0.5968,
      "step": 360
    },
    {
      "epoch": 0.07518033119983744,
      "grad_norm": 0.2819477617740631,
      "learning_rate": 0.0001631,
      "loss": 0.6058,
      "step": 370
    },
    {
      "epoch": 0.0772122320430763,
      "grad_norm": 0.31089890003204346,
      "learning_rate": 0.0001621,
      "loss": 0.5936,
      "step": 380
    },
    {
      "epoch": 0.07924413288631514,
      "grad_norm": 0.2927263081073761,
      "learning_rate": 0.0001611,
      "loss": 0.5973,
      "step": 390
    },
    {
      "epoch": 0.081276033729554,
      "grad_norm": 0.3094785213470459,
      "learning_rate": 0.00016010000000000002,
      "loss": 0.5933,
      "step": 400
    },
    {
      "epoch": 0.08330793457279284,
      "grad_norm": 0.2931379973888397,
      "learning_rate": 0.0001591,
      "loss": 0.603,
      "step": 410
    },
    {
      "epoch": 0.0853398354160317,
      "grad_norm": 0.34786146879196167,
      "learning_rate": 0.0001581,
      "loss": 0.6085,
      "step": 420
    },
    {
      "epoch": 0.08737173625927054,
      "grad_norm": 0.3706814646720886,
      "learning_rate": 0.0001571,
      "loss": 0.5681,
      "step": 430
    },
    {
      "epoch": 0.0894036371025094,
      "grad_norm": 0.3423382341861725,
      "learning_rate": 0.0001561,
      "loss": 0.6183,
      "step": 440
    },
    {
      "epoch": 0.09143553794574824,
      "grad_norm": 0.35331991314888,
      "learning_rate": 0.0001551,
      "loss": 0.5878,
      "step": 450
    },
    {
      "epoch": 0.0934674387889871,
      "grad_norm": 0.34410351514816284,
      "learning_rate": 0.0001541,
      "loss": 0.5703,
      "step": 460
    },
    {
      "epoch": 0.09549933963222594,
      "grad_norm": 0.31511390209198,
      "learning_rate": 0.0001531,
      "loss": 0.5629,
      "step": 470
    },
    {
      "epoch": 0.0975312404754648,
      "grad_norm": 0.29708486795425415,
      "learning_rate": 0.0001521,
      "loss": 0.587,
      "step": 480
    },
    {
      "epoch": 0.09956314131870364,
      "grad_norm": 0.3439713418483734,
      "learning_rate": 0.0001511,
      "loss": 0.5612,
      "step": 490
    },
    {
      "epoch": 0.1015950421619425,
      "grad_norm": 0.3090642988681793,
      "learning_rate": 0.0001501,
      "loss": 0.5722,
      "step": 500
    },
    {
      "epoch": 0.10362694300518134,
      "grad_norm": 0.4985158145427704,
      "learning_rate": 0.00014910000000000002,
      "loss": 0.5523,
      "step": 510
    },
    {
      "epoch": 0.1056588438484202,
      "grad_norm": 0.3066276013851166,
      "learning_rate": 0.00014810000000000002,
      "loss": 0.5552,
      "step": 520
    },
    {
      "epoch": 0.10769074469165904,
      "grad_norm": 0.3352774977684021,
      "learning_rate": 0.00014710000000000002,
      "loss": 0.5762,
      "step": 530
    },
    {
      "epoch": 0.1097226455348979,
      "grad_norm": 0.30998992919921875,
      "learning_rate": 0.00014610000000000003,
      "loss": 0.5474,
      "step": 540
    },
    {
      "epoch": 0.11175454637813674,
      "grad_norm": 0.3066215515136719,
      "learning_rate": 0.0001451,
      "loss": 0.5415,
      "step": 550
    },
    {
      "epoch": 0.1137864472213756,
      "grad_norm": 0.350922167301178,
      "learning_rate": 0.0001441,
      "loss": 0.5681,
      "step": 560
    },
    {
      "epoch": 0.11581834806461444,
      "grad_norm": 0.2955167591571808,
      "learning_rate": 0.0001431,
      "loss": 0.5389,
      "step": 570
    },
    {
      "epoch": 0.1178502489078533,
      "grad_norm": 0.343240350484848,
      "learning_rate": 0.0001421,
      "loss": 0.5659,
      "step": 580
    },
    {
      "epoch": 0.11988214975109214,
      "grad_norm": 0.3549663722515106,
      "learning_rate": 0.00014110000000000001,
      "loss": 0.5229,
      "step": 590
    },
    {
      "epoch": 0.121914050594331,
      "grad_norm": 0.35778629779815674,
      "learning_rate": 0.00014010000000000002,
      "loss": 0.5287,
      "step": 600
    },
    {
      "epoch": 0.12394595143756984,
      "grad_norm": 0.324738472700119,
      "learning_rate": 0.00013910000000000002,
      "loss": 0.5279,
      "step": 610
    },
    {
      "epoch": 0.12597785228080868,
      "grad_norm": 0.341294527053833,
      "learning_rate": 0.0001381,
      "loss": 0.5541,
      "step": 620
    },
    {
      "epoch": 0.12800975312404755,
      "grad_norm": 0.3240823745727539,
      "learning_rate": 0.0001371,
      "loss": 0.5237,
      "step": 630
    },
    {
      "epoch": 0.1300416539672864,
      "grad_norm": 0.3296601176261902,
      "learning_rate": 0.0001361,
      "loss": 0.5347,
      "step": 640
    },
    {
      "epoch": 0.13207355481052524,
      "grad_norm": 0.35458630323410034,
      "learning_rate": 0.0001351,
      "loss": 0.5406,
      "step": 650
    },
    {
      "epoch": 0.13410545565376408,
      "grad_norm": 0.3382067084312439,
      "learning_rate": 0.0001341,
      "loss": 0.508,
      "step": 660
    },
    {
      "epoch": 0.13613735649700295,
      "grad_norm": 0.2881470024585724,
      "learning_rate": 0.0001331,
      "loss": 0.5537,
      "step": 670
    },
    {
      "epoch": 0.1381692573402418,
      "grad_norm": 0.3398583233356476,
      "learning_rate": 0.0001321,
      "loss": 0.5309,
      "step": 680
    },
    {
      "epoch": 0.14020115818348064,
      "grad_norm": 0.3592161536216736,
      "learning_rate": 0.0001311,
      "loss": 0.5434,
      "step": 690
    },
    {
      "epoch": 0.14223305902671948,
      "grad_norm": 0.33346813917160034,
      "learning_rate": 0.0001301,
      "loss": 0.5502,
      "step": 700
    },
    {
      "epoch": 0.14426495986995835,
      "grad_norm": 0.3308549225330353,
      "learning_rate": 0.0001291,
      "loss": 0.5266,
      "step": 710
    },
    {
      "epoch": 0.1462968607131972,
      "grad_norm": 0.30425143241882324,
      "learning_rate": 0.0001281,
      "loss": 0.5372,
      "step": 720
    },
    {
      "epoch": 0.14832876155643604,
      "grad_norm": 0.32624122500419617,
      "learning_rate": 0.0001271,
      "loss": 0.5251,
      "step": 730
    },
    {
      "epoch": 0.15036066239967488,
      "grad_norm": 0.29890722036361694,
      "learning_rate": 0.0001261,
      "loss": 0.5167,
      "step": 740
    },
    {
      "epoch": 0.15239256324291375,
      "grad_norm": 0.32432568073272705,
      "learning_rate": 0.0001251,
      "loss": 0.5164,
      "step": 750
    },
    {
      "epoch": 0.1544244640861526,
      "grad_norm": 0.3122817873954773,
      "learning_rate": 0.0001241,
      "loss": 0.5206,
      "step": 760
    },
    {
      "epoch": 0.15645636492939144,
      "grad_norm": 0.29257136583328247,
      "learning_rate": 0.0001231,
      "loss": 0.4882,
      "step": 770
    },
    {
      "epoch": 0.15848826577263028,
      "grad_norm": 0.29955846071243286,
      "learning_rate": 0.0001221,
      "loss": 0.5078,
      "step": 780
    },
    {
      "epoch": 0.16052016661586915,
      "grad_norm": 0.31532934308052063,
      "learning_rate": 0.00012110000000000002,
      "loss": 0.5043,
      "step": 790
    },
    {
      "epoch": 0.162552067459108,
      "grad_norm": 0.3819871246814728,
      "learning_rate": 0.00012010000000000002,
      "loss": 0.5017,
      "step": 800
    },
    {
      "epoch": 0.16458396830234684,
      "grad_norm": 0.3124096989631653,
      "learning_rate": 0.00011910000000000001,
      "loss": 0.5031,
      "step": 810
    },
    {
      "epoch": 0.16661586914558568,
      "grad_norm": 0.3176095187664032,
      "learning_rate": 0.00011810000000000001,
      "loss": 0.4879,
      "step": 820
    },
    {
      "epoch": 0.16864776998882455,
      "grad_norm": 0.3287898600101471,
      "learning_rate": 0.00011710000000000001,
      "loss": 0.4966,
      "step": 830
    },
    {
      "epoch": 0.1706796708320634,
      "grad_norm": 0.35110652446746826,
      "learning_rate": 0.0001161,
      "loss": 0.511,
      "step": 840
    },
    {
      "epoch": 0.17271157167530224,
      "grad_norm": 0.3459363877773285,
      "learning_rate": 0.0001151,
      "loss": 0.4909,
      "step": 850
    },
    {
      "epoch": 0.17474347251854108,
      "grad_norm": 0.2982173562049866,
      "learning_rate": 0.00011410000000000001,
      "loss": 0.4951,
      "step": 860
    },
    {
      "epoch": 0.17677537336177995,
      "grad_norm": 0.32248011231422424,
      "learning_rate": 0.00011310000000000001,
      "loss": 0.499,
      "step": 870
    },
    {
      "epoch": 0.1788072742050188,
      "grad_norm": 0.3315400183200836,
      "learning_rate": 0.0001121,
      "loss": 0.5043,
      "step": 880
    },
    {
      "epoch": 0.18083917504825764,
      "grad_norm": 0.32426345348358154,
      "learning_rate": 0.0001111,
      "loss": 0.4906,
      "step": 890
    },
    {
      "epoch": 0.18287107589149648,
      "grad_norm": 0.4291764497756958,
      "learning_rate": 0.0001101,
      "loss": 0.4772,
      "step": 900
    },
    {
      "epoch": 0.18490297673473535,
      "grad_norm": 0.33165305852890015,
      "learning_rate": 0.0001091,
      "loss": 0.4812,
      "step": 910
    },
    {
      "epoch": 0.1869348775779742,
      "grad_norm": 0.2894167900085449,
      "learning_rate": 0.0001081,
      "loss": 0.5059,
      "step": 920
    },
    {
      "epoch": 0.18896677842121304,
      "grad_norm": 0.33602824807167053,
      "learning_rate": 0.0001071,
      "loss": 0.4763,
      "step": 930
    },
    {
      "epoch": 0.19099867926445188,
      "grad_norm": 0.3814552426338196,
      "learning_rate": 0.0001061,
      "loss": 0.4948,
      "step": 940
    },
    {
      "epoch": 0.19303058010769075,
      "grad_norm": 0.2832641303539276,
      "learning_rate": 0.0001051,
      "loss": 0.4785,
      "step": 950
    },
    {
      "epoch": 0.1950624809509296,
      "grad_norm": 0.3145245909690857,
      "learning_rate": 0.0001041,
      "loss": 0.4989,
      "step": 960
    },
    {
      "epoch": 0.19709438179416844,
      "grad_norm": 0.3255104422569275,
      "learning_rate": 0.0001031,
      "loss": 0.4888,
      "step": 970
    },
    {
      "epoch": 0.19912628263740728,
      "grad_norm": 0.37504690885543823,
      "learning_rate": 0.0001021,
      "loss": 0.4776,
      "step": 980
    },
    {
      "epoch": 0.20115818348064615,
      "grad_norm": 0.37614187598228455,
      "learning_rate": 0.00010109999999999999,
      "loss": 0.4891,
      "step": 990
    },
    {
      "epoch": 0.203190084323885,
      "grad_norm": 0.35442054271698,
      "learning_rate": 0.0001001,
      "loss": 0.4651,
      "step": 1000
    },
    {
      "epoch": 0.20522198516712384,
      "grad_norm": 0.30287984013557434,
      "learning_rate": 9.910000000000001e-05,
      "loss": 0.4667,
      "step": 1010
    },
    {
      "epoch": 0.20725388601036268,
      "grad_norm": 0.4503040313720703,
      "learning_rate": 9.81e-05,
      "loss": 0.4636,
      "step": 1020
    },
    {
      "epoch": 0.20928578685360155,
      "grad_norm": 0.2829679250717163,
      "learning_rate": 9.71e-05,
      "loss": 0.4735,
      "step": 1030
    },
    {
      "epoch": 0.2113176876968404,
      "grad_norm": 0.2916596829891205,
      "learning_rate": 9.61e-05,
      "loss": 0.4499,
      "step": 1040
    },
    {
      "epoch": 0.21334958854007924,
      "grad_norm": 0.3193717896938324,
      "learning_rate": 9.51e-05,
      "loss": 0.466,
      "step": 1050
    },
    {
      "epoch": 0.21538148938331808,
      "grad_norm": 0.3514256477355957,
      "learning_rate": 9.41e-05,
      "loss": 0.4895,
      "step": 1060
    },
    {
      "epoch": 0.21741339022655695,
      "grad_norm": 0.3160680830478668,
      "learning_rate": 9.310000000000001e-05,
      "loss": 0.4847,
      "step": 1070
    },
    {
      "epoch": 0.2194452910697958,
      "grad_norm": 0.3310452103614807,
      "learning_rate": 9.21e-05,
      "loss": 0.464,
      "step": 1080
    },
    {
      "epoch": 0.22147719191303464,
      "grad_norm": 0.3085184097290039,
      "learning_rate": 9.11e-05,
      "loss": 0.4557,
      "step": 1090
    },
    {
      "epoch": 0.22350909275627348,
      "grad_norm": 0.31938889622688293,
      "learning_rate": 9.010000000000001e-05,
      "loss": 0.4801,
      "step": 1100
    },
    {
      "epoch": 0.22554099359951235,
      "grad_norm": 0.2938026785850525,
      "learning_rate": 8.910000000000001e-05,
      "loss": 0.4568,
      "step": 1110
    },
    {
      "epoch": 0.2275728944427512,
      "grad_norm": 0.3140549659729004,
      "learning_rate": 8.81e-05,
      "loss": 0.4655,
      "step": 1120
    },
    {
      "epoch": 0.22960479528599004,
      "grad_norm": 0.33527278900146484,
      "learning_rate": 8.71e-05,
      "loss": 0.4689,
      "step": 1130
    },
    {
      "epoch": 0.23163669612922888,
      "grad_norm": 0.3277941942214966,
      "learning_rate": 8.61e-05,
      "loss": 0.4581,
      "step": 1140
    },
    {
      "epoch": 0.23366859697246775,
      "grad_norm": 0.3056517541408539,
      "learning_rate": 8.510000000000001e-05,
      "loss": 0.4547,
      "step": 1150
    },
    {
      "epoch": 0.2357004978157066,
      "grad_norm": 0.39737406373023987,
      "learning_rate": 8.41e-05,
      "loss": 0.4446,
      "step": 1160
    },
    {
      "epoch": 0.23773239865894544,
      "grad_norm": 0.3652009069919586,
      "learning_rate": 8.31e-05,
      "loss": 0.468,
      "step": 1170
    },
    {
      "epoch": 0.23976429950218428,
      "grad_norm": 0.33510997891426086,
      "learning_rate": 8.21e-05,
      "loss": 0.4478,
      "step": 1180
    },
    {
      "epoch": 0.24179620034542315,
      "grad_norm": 0.34428152441978455,
      "learning_rate": 8.11e-05,
      "loss": 0.4638,
      "step": 1190
    },
    {
      "epoch": 0.243828101188662,
      "grad_norm": 0.31807389855384827,
      "learning_rate": 8.010000000000001e-05,
      "loss": 0.4502,
      "step": 1200
    },
    {
      "epoch": 0.24586000203190084,
      "grad_norm": 0.2955916225910187,
      "learning_rate": 7.910000000000001e-05,
      "loss": 0.4466,
      "step": 1210
    },
    {
      "epoch": 0.24789190287513968,
      "grad_norm": 0.44328558444976807,
      "learning_rate": 7.81e-05,
      "loss": 0.4501,
      "step": 1220
    },
    {
      "epoch": 0.24992380371837855,
      "grad_norm": 0.30240997672080994,
      "learning_rate": 7.71e-05,
      "loss": 0.4563,
      "step": 1230
    },
    {
      "epoch": 0.25195570456161737,
      "grad_norm": 0.3203812539577484,
      "learning_rate": 7.61e-05,
      "loss": 0.4453,
      "step": 1240
    },
    {
      "epoch": 0.25398760540485626,
      "grad_norm": 0.291883260011673,
      "learning_rate": 7.510000000000001e-05,
      "loss": 0.4504,
      "step": 1250
    },
    {
      "epoch": 0.2560195062480951,
      "grad_norm": 0.3783104717731476,
      "learning_rate": 7.41e-05,
      "loss": 0.4508,
      "step": 1260
    },
    {
      "epoch": 0.25805140709133395,
      "grad_norm": 0.32842281460762024,
      "learning_rate": 7.31e-05,
      "loss": 0.4395,
      "step": 1270
    },
    {
      "epoch": 0.2600833079345728,
      "grad_norm": 0.367610901594162,
      "learning_rate": 7.21e-05,
      "loss": 0.4508,
      "step": 1280
    },
    {
      "epoch": 0.26211520877781164,
      "grad_norm": 0.31442612409591675,
      "learning_rate": 7.11e-05,
      "loss": 0.4192,
      "step": 1290
    },
    {
      "epoch": 0.2641471096210505,
      "grad_norm": 0.29231029748916626,
      "learning_rate": 7.01e-05,
      "loss": 0.4312,
      "step": 1300
    },
    {
      "epoch": 0.2661790104642893,
      "grad_norm": 0.3820737302303314,
      "learning_rate": 6.91e-05,
      "loss": 0.4319,
      "step": 1310
    },
    {
      "epoch": 0.26821091130752817,
      "grad_norm": 0.296101838350296,
      "learning_rate": 6.81e-05,
      "loss": 0.421,
      "step": 1320
    },
    {
      "epoch": 0.27024281215076706,
      "grad_norm": 0.31096476316452026,
      "learning_rate": 6.71e-05,
      "loss": 0.4357,
      "step": 1330
    },
    {
      "epoch": 0.2722747129940059,
      "grad_norm": 0.3151632249355316,
      "learning_rate": 6.610000000000001e-05,
      "loss": 0.4423,
      "step": 1340
    },
    {
      "epoch": 0.27430661383724475,
      "grad_norm": 0.36085644364356995,
      "learning_rate": 6.510000000000001e-05,
      "loss": 0.4217,
      "step": 1350
    },
    {
      "epoch": 0.2763385146804836,
      "grad_norm": 0.2855817675590515,
      "learning_rate": 6.41e-05,
      "loss": 0.4543,
      "step": 1360
    },
    {
      "epoch": 0.27837041552372244,
      "grad_norm": 0.315340518951416,
      "learning_rate": 6.31e-05,
      "loss": 0.4432,
      "step": 1370
    },
    {
      "epoch": 0.2804023163669613,
      "grad_norm": 0.334889680147171,
      "learning_rate": 6.21e-05,
      "loss": 0.4354,
      "step": 1380
    },
    {
      "epoch": 0.2824342172102001,
      "grad_norm": 0.32422834634780884,
      "learning_rate": 6.110000000000001e-05,
      "loss": 0.4254,
      "step": 1390
    },
    {
      "epoch": 0.28446611805343897,
      "grad_norm": 0.36255335807800293,
      "learning_rate": 6.0100000000000004e-05,
      "loss": 0.4412,
      "step": 1400
    },
    {
      "epoch": 0.28649801889667786,
      "grad_norm": 0.28405895829200745,
      "learning_rate": 5.91e-05,
      "loss": 0.443,
      "step": 1410
    },
    {
      "epoch": 0.2885299197399167,
      "grad_norm": 0.36841756105422974,
      "learning_rate": 5.8099999999999996e-05,
      "loss": 0.4255,
      "step": 1420
    },
    {
      "epoch": 0.29056182058315555,
      "grad_norm": 0.31047070026397705,
      "learning_rate": 5.71e-05,
      "loss": 0.4252,
      "step": 1430
    },
    {
      "epoch": 0.2925937214263944,
      "grad_norm": 0.3027880787849426,
      "learning_rate": 5.610000000000001e-05,
      "loss": 0.4184,
      "step": 1440
    },
    {
      "epoch": 0.29462562226963324,
      "grad_norm": 0.3042123317718506,
      "learning_rate": 5.5100000000000004e-05,
      "loss": 0.4294,
      "step": 1450
    },
    {
      "epoch": 0.2966575231128721,
      "grad_norm": 0.31831541657447815,
      "learning_rate": 5.410000000000001e-05,
      "loss": 0.4276,
      "step": 1460
    },
    {
      "epoch": 0.2986894239561109,
      "grad_norm": 0.35209330916404724,
      "learning_rate": 5.31e-05,
      "loss": 0.4319,
      "step": 1470
    },
    {
      "epoch": 0.30072132479934977,
      "grad_norm": 0.2997131943702698,
      "learning_rate": 5.2100000000000006e-05,
      "loss": 0.4294,
      "step": 1480
    },
    {
      "epoch": 0.30275322564258866,
      "grad_norm": 0.5217218399047852,
      "learning_rate": 5.11e-05,
      "loss": 0.4429,
      "step": 1490
    },
    {
      "epoch": 0.3047851264858275,
      "grad_norm": 0.27398842573165894,
      "learning_rate": 5.0100000000000005e-05,
      "loss": 0.43,
      "step": 1500
    },
    {
      "epoch": 0.30681702732906635,
      "grad_norm": 0.3113037943840027,
      "learning_rate": 4.91e-05,
      "loss": 0.4251,
      "step": 1510
    },
    {
      "epoch": 0.3088489281723052,
      "grad_norm": 0.3125740587711334,
      "learning_rate": 4.8100000000000004e-05,
      "loss": 0.4421,
      "step": 1520
    },
    {
      "epoch": 0.31088082901554404,
      "grad_norm": 0.29475459456443787,
      "learning_rate": 4.71e-05,
      "loss": 0.4079,
      "step": 1530
    },
    {
      "epoch": 0.3129127298587829,
      "grad_norm": 0.309601753950119,
      "learning_rate": 4.61e-05,
      "loss": 0.412,
      "step": 1540
    },
    {
      "epoch": 0.3149446307020217,
      "grad_norm": 0.32328686118125916,
      "learning_rate": 4.5100000000000005e-05,
      "loss": 0.4171,
      "step": 1550
    },
    {
      "epoch": 0.31697653154526056,
      "grad_norm": 0.3305211663246155,
      "learning_rate": 4.41e-05,
      "loss": 0.4123,
      "step": 1560
    },
    {
      "epoch": 0.31900843238849946,
      "grad_norm": 0.2884441912174225,
      "learning_rate": 4.3100000000000004e-05,
      "loss": 0.4388,
      "step": 1570
    },
    {
      "epoch": 0.3210403332317383,
      "grad_norm": 0.42773133516311646,
      "learning_rate": 4.21e-05,
      "loss": 0.419,
      "step": 1580
    },
    {
      "epoch": 0.32307223407497715,
      "grad_norm": 0.28572404384613037,
      "learning_rate": 4.11e-05,
      "loss": 0.4116,
      "step": 1590
    },
    {
      "epoch": 0.325104134918216,
      "grad_norm": 0.3362528085708618,
      "learning_rate": 4.0100000000000006e-05,
      "loss": 0.3963,
      "step": 1600
    },
    {
      "epoch": 0.32713603576145484,
      "grad_norm": 0.3202821612358093,
      "learning_rate": 3.91e-05,
      "loss": 0.4447,
      "step": 1610
    },
    {
      "epoch": 0.3291679366046937,
      "grad_norm": 0.33235105872154236,
      "learning_rate": 3.8100000000000005e-05,
      "loss": 0.4101,
      "step": 1620
    },
    {
      "epoch": 0.3311998374479325,
      "grad_norm": 0.2858361601829529,
      "learning_rate": 3.71e-05,
      "loss": 0.4207,
      "step": 1630
    },
    {
      "epoch": 0.33323173829117136,
      "grad_norm": 0.3629436492919922,
      "learning_rate": 3.61e-05,
      "loss": 0.4239,
      "step": 1640
    },
    {
      "epoch": 0.33526363913441026,
      "grad_norm": 0.35089170932769775,
      "learning_rate": 3.51e-05,
      "loss": 0.422,
      "step": 1650
    },
    {
      "epoch": 0.3372955399776491,
      "grad_norm": 0.3097570836544037,
      "learning_rate": 3.41e-05,
      "loss": 0.4161,
      "step": 1660
    },
    {
      "epoch": 0.33932744082088795,
      "grad_norm": 0.3087622821331024,
      "learning_rate": 3.3100000000000005e-05,
      "loss": 0.403,
      "step": 1670
    },
    {
      "epoch": 0.3413593416641268,
      "grad_norm": 0.3576933741569519,
      "learning_rate": 3.21e-05,
      "loss": 0.4235,
      "step": 1680
    },
    {
      "epoch": 0.34339124250736563,
      "grad_norm": 0.3173774480819702,
      "learning_rate": 3.1100000000000004e-05,
      "loss": 0.4152,
      "step": 1690
    },
    {
      "epoch": 0.3454231433506045,
      "grad_norm": 0.353092759847641,
      "learning_rate": 3.01e-05,
      "loss": 0.4054,
      "step": 1700
    },
    {
      "epoch": 0.3474550441938433,
      "grad_norm": 0.33786725997924805,
      "learning_rate": 2.91e-05,
      "loss": 0.413,
      "step": 1710
    },
    {
      "epoch": 0.34948694503708216,
      "grad_norm": 0.3429342806339264,
      "learning_rate": 2.8100000000000005e-05,
      "loss": 0.4171,
      "step": 1720
    },
    {
      "epoch": 0.35151884588032106,
      "grad_norm": 0.33753132820129395,
      "learning_rate": 2.7100000000000005e-05,
      "loss": 0.4157,
      "step": 1730
    },
    {
      "epoch": 0.3535507467235599,
      "grad_norm": 0.35612058639526367,
      "learning_rate": 2.61e-05,
      "loss": 0.4165,
      "step": 1740
    },
    {
      "epoch": 0.35558264756679875,
      "grad_norm": 0.318652868270874,
      "learning_rate": 2.51e-05,
      "loss": 0.4091,
      "step": 1750
    },
    {
      "epoch": 0.3576145484100376,
      "grad_norm": 0.36677148938179016,
      "learning_rate": 2.41e-05,
      "loss": 0.4085,
      "step": 1760
    },
    {
      "epoch": 0.35964644925327643,
      "grad_norm": 0.3742050528526306,
      "learning_rate": 2.3100000000000002e-05,
      "loss": 0.4307,
      "step": 1770
    },
    {
      "epoch": 0.3616783500965153,
      "grad_norm": 0.403211385011673,
      "learning_rate": 2.2100000000000002e-05,
      "loss": 0.4167,
      "step": 1780
    },
    {
      "epoch": 0.3637102509397541,
      "grad_norm": 0.30064159631729126,
      "learning_rate": 2.11e-05,
      "loss": 0.4208,
      "step": 1790
    },
    {
      "epoch": 0.36574215178299296,
      "grad_norm": 0.3409578800201416,
      "learning_rate": 2.01e-05,
      "loss": 0.4229,
      "step": 1800
    }
  ],
  "logging_steps": 10,
  "max_steps": 2000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.022355193271419e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
