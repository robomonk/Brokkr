{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.203190084323885,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00203190084323885,
      "grad_norm": 1.165687084197998,
      "learning_rate": 0.00019910000000000001,
      "loss": 1.9569,
      "step": 10
    },
    {
      "epoch": 0.0040638016864777,
      "grad_norm": 0.2793021500110626,
      "learning_rate": 0.00019810000000000002,
      "loss": 1.0785,
      "step": 20
    },
    {
      "epoch": 0.00609570252971655,
      "grad_norm": 0.2917936444282532,
      "learning_rate": 0.00019710000000000002,
      "loss": 0.9965,
      "step": 30
    },
    {
      "epoch": 0.0081276033729554,
      "grad_norm": 0.3397102355957031,
      "learning_rate": 0.00019610000000000002,
      "loss": 0.9243,
      "step": 40
    },
    {
      "epoch": 0.01015950421619425,
      "grad_norm": 0.2767530083656311,
      "learning_rate": 0.00019510000000000003,
      "loss": 0.8793,
      "step": 50
    },
    {
      "epoch": 0.0121914050594331,
      "grad_norm": 0.26831814646720886,
      "learning_rate": 0.0001941,
      "loss": 0.8664,
      "step": 60
    },
    {
      "epoch": 0.01422330590267195,
      "grad_norm": 0.24407950043678284,
      "learning_rate": 0.0001931,
      "loss": 0.8438,
      "step": 70
    },
    {
      "epoch": 0.0162552067459108,
      "grad_norm": 0.29952120780944824,
      "learning_rate": 0.0001921,
      "loss": 0.7991,
      "step": 80
    },
    {
      "epoch": 0.01828710758914965,
      "grad_norm": 0.25862154364585876,
      "learning_rate": 0.0001911,
      "loss": 0.7997,
      "step": 90
    },
    {
      "epoch": 0.0203190084323885,
      "grad_norm": 0.25606852769851685,
      "learning_rate": 0.0001901,
      "loss": 0.8306,
      "step": 100
    },
    {
      "epoch": 0.02235090927562735,
      "grad_norm": 0.2541874647140503,
      "learning_rate": 0.00018910000000000002,
      "loss": 0.7945,
      "step": 110
    },
    {
      "epoch": 0.0243828101188662,
      "grad_norm": 0.29285746812820435,
      "learning_rate": 0.00018810000000000002,
      "loss": 0.7923,
      "step": 120
    },
    {
      "epoch": 0.02641471096210505,
      "grad_norm": 0.24532675743103027,
      "learning_rate": 0.00018710000000000002,
      "loss": 0.7397,
      "step": 130
    },
    {
      "epoch": 0.0284466118053439,
      "grad_norm": 0.2939009964466095,
      "learning_rate": 0.0001861,
      "loss": 0.7454,
      "step": 140
    },
    {
      "epoch": 0.03047851264858275,
      "grad_norm": 0.28754323720932007,
      "learning_rate": 0.0001851,
      "loss": 0.7084,
      "step": 150
    },
    {
      "epoch": 0.0325104134918216,
      "grad_norm": 0.2901480495929718,
      "learning_rate": 0.0001841,
      "loss": 0.7442,
      "step": 160
    },
    {
      "epoch": 0.03454231433506045,
      "grad_norm": 0.34662312269210815,
      "learning_rate": 0.0001831,
      "loss": 0.7286,
      "step": 170
    },
    {
      "epoch": 0.0365742151782993,
      "grad_norm": 0.306060791015625,
      "learning_rate": 0.0001821,
      "loss": 0.7227,
      "step": 180
    },
    {
      "epoch": 0.03860611602153815,
      "grad_norm": 0.2749321162700653,
      "learning_rate": 0.0001811,
      "loss": 0.7239,
      "step": 190
    },
    {
      "epoch": 0.040638016864777,
      "grad_norm": 0.327809602022171,
      "learning_rate": 0.00018010000000000001,
      "loss": 0.7055,
      "step": 200
    },
    {
      "epoch": 0.04266991770801585,
      "grad_norm": 0.2977617084980011,
      "learning_rate": 0.0001791,
      "loss": 0.6874,
      "step": 210
    },
    {
      "epoch": 0.0447018185512547,
      "grad_norm": 0.29844924807548523,
      "learning_rate": 0.0001781,
      "loss": 0.7184,
      "step": 220
    },
    {
      "epoch": 0.04673371939449355,
      "grad_norm": 0.27778360247612,
      "learning_rate": 0.0001771,
      "loss": 0.6679,
      "step": 230
    },
    {
      "epoch": 0.0487656202377324,
      "grad_norm": 0.284200519323349,
      "learning_rate": 0.0001761,
      "loss": 0.6643,
      "step": 240
    },
    {
      "epoch": 0.05079752108097125,
      "grad_norm": 0.29384177923202515,
      "learning_rate": 0.0001751,
      "loss": 0.6905,
      "step": 250
    },
    {
      "epoch": 0.0528294219242101,
      "grad_norm": 0.29748770594596863,
      "learning_rate": 0.00017410000000000003,
      "loss": 0.6557,
      "step": 260
    },
    {
      "epoch": 0.05486132276744895,
      "grad_norm": 0.2729402482509613,
      "learning_rate": 0.0001731,
      "loss": 0.6382,
      "step": 270
    },
    {
      "epoch": 0.0568932236106878,
      "grad_norm": 0.2993599474430084,
      "learning_rate": 0.0001721,
      "loss": 0.6568,
      "step": 280
    },
    {
      "epoch": 0.05892512445392665,
      "grad_norm": 0.2952386438846588,
      "learning_rate": 0.0001711,
      "loss": 0.6362,
      "step": 290
    },
    {
      "epoch": 0.0609570252971655,
      "grad_norm": 0.26254668831825256,
      "learning_rate": 0.00017010000000000001,
      "loss": 0.6027,
      "step": 300
    },
    {
      "epoch": 0.06298892614040434,
      "grad_norm": 0.35756734013557434,
      "learning_rate": 0.00016910000000000002,
      "loss": 0.6209,
      "step": 310
    },
    {
      "epoch": 0.0650208269836432,
      "grad_norm": 0.331665962934494,
      "learning_rate": 0.00016810000000000002,
      "loss": 0.63,
      "step": 320
    },
    {
      "epoch": 0.06705272782688204,
      "grad_norm": 0.32876911759376526,
      "learning_rate": 0.00016710000000000002,
      "loss": 0.6053,
      "step": 330
    },
    {
      "epoch": 0.0690846286701209,
      "grad_norm": 0.2975154519081116,
      "learning_rate": 0.0001661,
      "loss": 0.6132,
      "step": 340
    },
    {
      "epoch": 0.07111652951335974,
      "grad_norm": 0.32871952652931213,
      "learning_rate": 0.0001651,
      "loss": 0.6056,
      "step": 350
    },
    {
      "epoch": 0.0731484303565986,
      "grad_norm": 0.3313707411289215,
      "learning_rate": 0.0001641,
      "loss": 0.5968,
      "step": 360
    },
    {
      "epoch": 0.07518033119983744,
      "grad_norm": 0.2819477617740631,
      "learning_rate": 0.0001631,
      "loss": 0.6058,
      "step": 370
    },
    {
      "epoch": 0.0772122320430763,
      "grad_norm": 0.31089890003204346,
      "learning_rate": 0.0001621,
      "loss": 0.5936,
      "step": 380
    },
    {
      "epoch": 0.07924413288631514,
      "grad_norm": 0.2927263081073761,
      "learning_rate": 0.0001611,
      "loss": 0.5973,
      "step": 390
    },
    {
      "epoch": 0.081276033729554,
      "grad_norm": 0.3094785213470459,
      "learning_rate": 0.00016010000000000002,
      "loss": 0.5933,
      "step": 400
    },
    {
      "epoch": 0.08330793457279284,
      "grad_norm": 0.2931379973888397,
      "learning_rate": 0.0001591,
      "loss": 0.603,
      "step": 410
    },
    {
      "epoch": 0.0853398354160317,
      "grad_norm": 0.34786146879196167,
      "learning_rate": 0.0001581,
      "loss": 0.6085,
      "step": 420
    },
    {
      "epoch": 0.08737173625927054,
      "grad_norm": 0.3706814646720886,
      "learning_rate": 0.0001571,
      "loss": 0.5681,
      "step": 430
    },
    {
      "epoch": 0.0894036371025094,
      "grad_norm": 0.3423382341861725,
      "learning_rate": 0.0001561,
      "loss": 0.6183,
      "step": 440
    },
    {
      "epoch": 0.09143553794574824,
      "grad_norm": 0.35331991314888,
      "learning_rate": 0.0001551,
      "loss": 0.5878,
      "step": 450
    },
    {
      "epoch": 0.0934674387889871,
      "grad_norm": 0.34410351514816284,
      "learning_rate": 0.0001541,
      "loss": 0.5703,
      "step": 460
    },
    {
      "epoch": 0.09549933963222594,
      "grad_norm": 0.31511390209198,
      "learning_rate": 0.0001531,
      "loss": 0.5629,
      "step": 470
    },
    {
      "epoch": 0.0975312404754648,
      "grad_norm": 0.29708486795425415,
      "learning_rate": 0.0001521,
      "loss": 0.587,
      "step": 480
    },
    {
      "epoch": 0.09956314131870364,
      "grad_norm": 0.3439713418483734,
      "learning_rate": 0.0001511,
      "loss": 0.5612,
      "step": 490
    },
    {
      "epoch": 0.1015950421619425,
      "grad_norm": 0.3090642988681793,
      "learning_rate": 0.0001501,
      "loss": 0.5722,
      "step": 500
    },
    {
      "epoch": 0.10362694300518134,
      "grad_norm": 0.4985158145427704,
      "learning_rate": 0.00014910000000000002,
      "loss": 0.5523,
      "step": 510
    },
    {
      "epoch": 0.1056588438484202,
      "grad_norm": 0.3066276013851166,
      "learning_rate": 0.00014810000000000002,
      "loss": 0.5552,
      "step": 520
    },
    {
      "epoch": 0.10769074469165904,
      "grad_norm": 0.3352774977684021,
      "learning_rate": 0.00014710000000000002,
      "loss": 0.5762,
      "step": 530
    },
    {
      "epoch": 0.1097226455348979,
      "grad_norm": 0.30998992919921875,
      "learning_rate": 0.00014610000000000003,
      "loss": 0.5474,
      "step": 540
    },
    {
      "epoch": 0.11175454637813674,
      "grad_norm": 0.3066215515136719,
      "learning_rate": 0.0001451,
      "loss": 0.5415,
      "step": 550
    },
    {
      "epoch": 0.1137864472213756,
      "grad_norm": 0.350922167301178,
      "learning_rate": 0.0001441,
      "loss": 0.5681,
      "step": 560
    },
    {
      "epoch": 0.11581834806461444,
      "grad_norm": 0.2955167591571808,
      "learning_rate": 0.0001431,
      "loss": 0.5389,
      "step": 570
    },
    {
      "epoch": 0.1178502489078533,
      "grad_norm": 0.343240350484848,
      "learning_rate": 0.0001421,
      "loss": 0.5659,
      "step": 580
    },
    {
      "epoch": 0.11988214975109214,
      "grad_norm": 0.3549663722515106,
      "learning_rate": 0.00014110000000000001,
      "loss": 0.5229,
      "step": 590
    },
    {
      "epoch": 0.121914050594331,
      "grad_norm": 0.35778629779815674,
      "learning_rate": 0.00014010000000000002,
      "loss": 0.5287,
      "step": 600
    },
    {
      "epoch": 0.12394595143756984,
      "grad_norm": 0.324738472700119,
      "learning_rate": 0.00013910000000000002,
      "loss": 0.5279,
      "step": 610
    },
    {
      "epoch": 0.12597785228080868,
      "grad_norm": 0.341294527053833,
      "learning_rate": 0.0001381,
      "loss": 0.5541,
      "step": 620
    },
    {
      "epoch": 0.12800975312404755,
      "grad_norm": 0.3240823745727539,
      "learning_rate": 0.0001371,
      "loss": 0.5237,
      "step": 630
    },
    {
      "epoch": 0.1300416539672864,
      "grad_norm": 0.3296601176261902,
      "learning_rate": 0.0001361,
      "loss": 0.5347,
      "step": 640
    },
    {
      "epoch": 0.13207355481052524,
      "grad_norm": 0.35458630323410034,
      "learning_rate": 0.0001351,
      "loss": 0.5406,
      "step": 650
    },
    {
      "epoch": 0.13410545565376408,
      "grad_norm": 0.3382067084312439,
      "learning_rate": 0.0001341,
      "loss": 0.508,
      "step": 660
    },
    {
      "epoch": 0.13613735649700295,
      "grad_norm": 0.2881470024585724,
      "learning_rate": 0.0001331,
      "loss": 0.5537,
      "step": 670
    },
    {
      "epoch": 0.1381692573402418,
      "grad_norm": 0.3398583233356476,
      "learning_rate": 0.0001321,
      "loss": 0.5309,
      "step": 680
    },
    {
      "epoch": 0.14020115818348064,
      "grad_norm": 0.3592161536216736,
      "learning_rate": 0.0001311,
      "loss": 0.5434,
      "step": 690
    },
    {
      "epoch": 0.14223305902671948,
      "grad_norm": 0.33346813917160034,
      "learning_rate": 0.0001301,
      "loss": 0.5502,
      "step": 700
    },
    {
      "epoch": 0.14426495986995835,
      "grad_norm": 0.3308549225330353,
      "learning_rate": 0.0001291,
      "loss": 0.5266,
      "step": 710
    },
    {
      "epoch": 0.1462968607131972,
      "grad_norm": 0.30425143241882324,
      "learning_rate": 0.0001281,
      "loss": 0.5372,
      "step": 720
    },
    {
      "epoch": 0.14832876155643604,
      "grad_norm": 0.32624122500419617,
      "learning_rate": 0.0001271,
      "loss": 0.5251,
      "step": 730
    },
    {
      "epoch": 0.15036066239967488,
      "grad_norm": 0.29890722036361694,
      "learning_rate": 0.0001261,
      "loss": 0.5167,
      "step": 740
    },
    {
      "epoch": 0.15239256324291375,
      "grad_norm": 0.32432568073272705,
      "learning_rate": 0.0001251,
      "loss": 0.5164,
      "step": 750
    },
    {
      "epoch": 0.1544244640861526,
      "grad_norm": 0.3122817873954773,
      "learning_rate": 0.0001241,
      "loss": 0.5206,
      "step": 760
    },
    {
      "epoch": 0.15645636492939144,
      "grad_norm": 0.29257136583328247,
      "learning_rate": 0.0001231,
      "loss": 0.4882,
      "step": 770
    },
    {
      "epoch": 0.15848826577263028,
      "grad_norm": 0.29955846071243286,
      "learning_rate": 0.0001221,
      "loss": 0.5078,
      "step": 780
    },
    {
      "epoch": 0.16052016661586915,
      "grad_norm": 0.31532934308052063,
      "learning_rate": 0.00012110000000000002,
      "loss": 0.5043,
      "step": 790
    },
    {
      "epoch": 0.162552067459108,
      "grad_norm": 0.3819871246814728,
      "learning_rate": 0.00012010000000000002,
      "loss": 0.5017,
      "step": 800
    },
    {
      "epoch": 0.16458396830234684,
      "grad_norm": 0.3124096989631653,
      "learning_rate": 0.00011910000000000001,
      "loss": 0.5031,
      "step": 810
    },
    {
      "epoch": 0.16661586914558568,
      "grad_norm": 0.3176095187664032,
      "learning_rate": 0.00011810000000000001,
      "loss": 0.4879,
      "step": 820
    },
    {
      "epoch": 0.16864776998882455,
      "grad_norm": 0.3287898600101471,
      "learning_rate": 0.00011710000000000001,
      "loss": 0.4966,
      "step": 830
    },
    {
      "epoch": 0.1706796708320634,
      "grad_norm": 0.35110652446746826,
      "learning_rate": 0.0001161,
      "loss": 0.511,
      "step": 840
    },
    {
      "epoch": 0.17271157167530224,
      "grad_norm": 0.3459363877773285,
      "learning_rate": 0.0001151,
      "loss": 0.4909,
      "step": 850
    },
    {
      "epoch": 0.17474347251854108,
      "grad_norm": 0.2982173562049866,
      "learning_rate": 0.00011410000000000001,
      "loss": 0.4951,
      "step": 860
    },
    {
      "epoch": 0.17677537336177995,
      "grad_norm": 0.32248011231422424,
      "learning_rate": 0.00011310000000000001,
      "loss": 0.499,
      "step": 870
    },
    {
      "epoch": 0.1788072742050188,
      "grad_norm": 0.3315400183200836,
      "learning_rate": 0.0001121,
      "loss": 0.5043,
      "step": 880
    },
    {
      "epoch": 0.18083917504825764,
      "grad_norm": 0.32426345348358154,
      "learning_rate": 0.0001111,
      "loss": 0.4906,
      "step": 890
    },
    {
      "epoch": 0.18287107589149648,
      "grad_norm": 0.4291764497756958,
      "learning_rate": 0.0001101,
      "loss": 0.4772,
      "step": 900
    },
    {
      "epoch": 0.18490297673473535,
      "grad_norm": 0.33165305852890015,
      "learning_rate": 0.0001091,
      "loss": 0.4812,
      "step": 910
    },
    {
      "epoch": 0.1869348775779742,
      "grad_norm": 0.2894167900085449,
      "learning_rate": 0.0001081,
      "loss": 0.5059,
      "step": 920
    },
    {
      "epoch": 0.18896677842121304,
      "grad_norm": 0.33602824807167053,
      "learning_rate": 0.0001071,
      "loss": 0.4763,
      "step": 930
    },
    {
      "epoch": 0.19099867926445188,
      "grad_norm": 0.3814552426338196,
      "learning_rate": 0.0001061,
      "loss": 0.4948,
      "step": 940
    },
    {
      "epoch": 0.19303058010769075,
      "grad_norm": 0.2832641303539276,
      "learning_rate": 0.0001051,
      "loss": 0.4785,
      "step": 950
    },
    {
      "epoch": 0.1950624809509296,
      "grad_norm": 0.3145245909690857,
      "learning_rate": 0.0001041,
      "loss": 0.4989,
      "step": 960
    },
    {
      "epoch": 0.19709438179416844,
      "grad_norm": 0.3255104422569275,
      "learning_rate": 0.0001031,
      "loss": 0.4888,
      "step": 970
    },
    {
      "epoch": 0.19912628263740728,
      "grad_norm": 0.37504690885543823,
      "learning_rate": 0.0001021,
      "loss": 0.4776,
      "step": 980
    },
    {
      "epoch": 0.20115818348064615,
      "grad_norm": 0.37614187598228455,
      "learning_rate": 0.00010109999999999999,
      "loss": 0.4891,
      "step": 990
    },
    {
      "epoch": 0.203190084323885,
      "grad_norm": 0.35442054271698,
      "learning_rate": 0.0001001,
      "loss": 0.4651,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 2000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6790862184841216e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
